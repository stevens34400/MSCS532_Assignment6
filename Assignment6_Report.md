# Assignment 6 Report: Medians, Order Statistics & Elementary Data Structures

## Part 1: Selection Algorithms

In this part, we implemented and analyzed two algorithms for selecting the k-th smallest element in an array (1-based index): a randomized Quickselect variant and the deterministic Median of Medians method. The randomized Quickselect chooses a pivot uniformly at random, partitions the array into elements less than, equal to, and greater than the pivot, and recurses only into the segment containing the desired order statistic. This yields expected linear time behavior due to the high probability of reasonably balanced splits, although its worst case degrades to O(n²) when unlucky pivots are repeatedly chosen. The deterministic algorithm mitigates that risk by grouping the input into chunks of five, computing each group’s median, and recursively selecting the median of those medians to serve as a pivot. That pivot guarantees that a constant fraction of elements is discarded each recursion, producing a recurrence that resolves to worst-case O(n) time. Both implementations operate on copies of the input to avoid mutating caller data; their partitioning and recursion incur auxiliary space that is linear in the worst case, with the randomized version having expected recursion depth of O(log n) and the deterministic version incurring linear overhead from grouping and median computation.

Empirically, performance was compared across different input sizes (e.g., 1,000; 5,000; 10,000) and distributions (random, sorted, reverse-sorted), consistently selecting the median (k = n/2) for each test. High-resolution timers recorded separate runtimes for the randomized and deterministic procedures. The observed results reflect theoretical predictions: the deterministic method showed stable performance regardless of input ordering, evidencing its worst-case guarantee, while the randomized Quickselect was typically faster on random data due to lower constant overhead but exhibited higher variance and occasional slowdowns on adversarial-appearing inputs like sorted or reverse-sorted arrays when poor pivots were chosen. This contrast highlights the trade-off between predictability and practical average-case speed—choosing between the two depends on whether worst-case bounds or expected throughput matter more in the target application.

## Part 2: Elementary Data Structures

In the second part, we built elementary data structures from scratch to understand their operational mechanics and cost models. The dynamic array implementation supports insertion at arbitrary indices, deletion, and indexed access. It maintains an underlying fixed-size buffer that automatically doubles when full and halves when sparsely populated, giving amortized O(1) performance for end insertions while incurring O(n) cost for arbitrary positional insertions and deletions due to element shifting. The stack is implemented as a lightweight wrapper around a native Python list, exposing push, pop, peek, size, and emptiness checks with constant-time behavior for core operations. The queue achieves amortized O(1) enqueue and dequeue by combining a list with a head pointer and performing lazy cleanup (compacting the internal buffer once the head has advanced sufficiently) to avoid unbounded memory usage while keeping per-operation cost low.

These implementations illustrate classical trade-offs in data structure design: dynamic arrays provide fast random access but suffer on in-place mid-array modifications; stacks and queues built atop such primitives can be optimized for their expected usage patterns, with the queue’s lazy deletion balancing performance and space reclamation. Although the original assignment prompt also referenced linked lists (and optionally rooted trees), this submission focuses on arrays, stacks, and queues; extending to a singly linked list would further complement the comparison by providing O(1) insertion/deletion at arbitrary positions without shifting, at the cost of pointer overhead and lack of constant-time random access.

From a practical perspective, these foundational structures are building blocks for larger systems—dynamic arrays form the backbone of resizable collections, stacks underpin recursion emulation and expression parsing, and queues are essential for scheduling, breadth-first traversal, and buffering. The selection algorithms developed in Part 1 complement these by offering efficient mechanisms for order-statistics queries such as median computation, which is useful in statistical summaries, streaming data processing, and robust filtering.

## Conclusion

This assignment reinforced both algorithmic theory and implementation discipline. Part 1 contrasted two selection strategies, revealing when to favor deterministic guarantees versus expected-case performance, while Part 2 grounded the student in the concrete behaviors of core storage/access abstractions. Together, these components provide a strong foundation for designing and reasoning about performance-sensitive, data-intensive software systems.
